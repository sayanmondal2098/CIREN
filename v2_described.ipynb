{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95ad8d1",
   "metadata": {},
   "source": [
    "This notebook is a documented version of v2.ipynb. Each code cell below is preceded by a markdown description explaining the cell's purpose, inputs, and expected outputs. Use the descriptions to understand and run cells in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843290fc",
   "metadata": {},
   "source": [
    "Imports and environment setup: imports standard Python libraries used throughout the notebook, installs `pyvis` for interactive visualization, and prepares network and plotting libraries.\n",
    "\n",
    "Inputs: none.\n",
    "Outputs: imported modules available in the kernel and `pyvis` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4111107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Install pyvis for interactive visualization (optional but recommended)\n",
    "!pip install pyvis\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e23b0f",
   "metadata": {},
   "source": [
    "Repository cloning: clones the `CTI-HAL` repository to obtain example data files used later.\n",
    "\n",
    "Inputs: network access to GitHub.\n",
    "Outputs: local `CTI-HAL` directory with dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get the dataset\n",
    "!git clone https://github.com/dessertlab/CTI-HAL.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807aba39",
   "metadata": {},
   "source": [
    "Discover CTI report JSON files: sets the data path, recursively searches for `.json` files, and prints a small sample.\n",
    "\n",
    "Inputs: `CTI-HAL/data/` directory.\n",
    "Outputs: `files` list containing paths to CTI report JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace625e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CTI-HAL data directory\n",
    "data_path = './CTI-HAL/data/'\n",
    "\n",
    "# Recursively find all .json files in the CTI-HAL directory and its subdirectories\n",
    "files = []\n",
    "for root, dirs, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.json'):\n",
    "            files.append(os.path.join(root, filename))\n",
    "\n",
    "print(f\"Found {len(files)} CTI reports.\")\n",
    "print(\"\\nFirst few files found:\")\n",
    "for f in files[:5]:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0707946",
   "metadata": {},
   "source": [
    "Parse semantic triples: reads each CTI JSON report and extracts triples representing Actor→uses→Technique, Technique→belongs_to→Tactic, and Technique→utilizes→Tool.\n",
    "\n",
    "Inputs: `files` list and JSON schemas in CTI-HAL.\n",
    "Outputs: `actor_technique_triples`, `technique_tactic_triples`, `technique_tool_triples`, and combined `all_triples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parse Triples from CTI-HAL\n",
    "# CTI-HAL JSON structure contains: TACTIC, TECHNIQUE, TOOL, and CONTEXT (Actor)\n",
    "# We convert these into triples: (Subject, Relation, Object)\n",
    "\n",
    "triples = []\n",
    "actor_technique_triples = []  # Actor → uses → Technique\n",
    "technique_tactic_triples = []  # Technique → belongs_to → Tactic\n",
    "technique_tool_triples = []    # Technique → utilizes → Tool\n",
    "\n",
    "# Extract actor name from file path (e.g., apt29, carbanak, etc.)\n",
    "for file_path in files:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract actor from file path\n",
    "        path_parts = file_path.replace('\\\\', '/').split('/')\n",
    "        actor = path_parts[3] if len(path_parts) > 3 else 'UNKNOWN'\n",
    "        \n",
    "        # Process each entry in the JSON\n",
    "        for entry in data:\n",
    "            metadata = entry.get('metadata', {})\n",
    "            context = entry.get('context', '')\n",
    "            \n",
    "            # Extract tools and create Actor → uses → Technique triples\n",
    "            tool_names = metadata.get('tool_name', [])\n",
    "            tool_ids = metadata.get('tool', [])\n",
    "            \n",
    "            # Extract technique information\n",
    "            technique_id = entry.get('technique')\n",
    "            technique_name = metadata.get('technique_name')\n",
    "            \n",
    "            # Extract tactic information\n",
    "            tactic_names = metadata.get('tactic_name', [])\n",
    "            tactic_ids = metadata.get('tactic', [])\n",
    "            \n",
    "            # 1. Actor → uses → Technique\n",
    "            if technique_id and technique_name:\n",
    "                actor_technique_triples.append({\n",
    "                    'subject': actor.upper(),\n",
    "                    'subject_id': None,\n",
    "                    'relation': 'uses',\n",
    "                    'object': technique_name,\n",
    "                    'object_id': technique_id\n",
    "                })\n",
    "            \n",
    "            # 2. Technique → belongs_to → Tactic\n",
    "            if technique_id and technique_name:\n",
    "                for i, tactic_id in enumerate(tactic_ids):\n",
    "                    tactic_name = tactic_names[i] if i < len(tactic_names) else None\n",
    "                    if tactic_name:\n",
    "                        technique_tactic_triples.append({\n",
    "                            'subject': technique_name,\n",
    "                            'subject_id': technique_id,\n",
    "                            'relation': 'belongs_to',\n",
    "                            'object': tactic_name,\n",
    "                            'object_id': tactic_id\n",
    "                        })\n",
    "            \n",
    "            # 3. Technique → utilizes → Tool\n",
    "            if technique_id and technique_name:\n",
    "                for i, tool_id in enumerate(tool_ids):\n",
    "                    tool_name = tool_names[i] if i < len(tool_names) else None\n",
    "                    if tool_name:\n",
    "                        technique_tool_triples.append({\n",
    "                            'subject': technique_name,\n",
    "                            'subject_id': technique_id,\n",
    "                            'relation': 'utilizes',\n",
    "                            'object': tool_name,\n",
    "                            'object_id': tool_id\n",
    "                        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Combine all triples\n",
    "all_triples = actor_technique_triples + technique_tactic_triples + technique_tool_triples\n",
    "\n",
    "print(f\"Total triples extracted: {len(all_triples)}\")\n",
    "print(f\"  - Actor → uses → Technique: {len(actor_technique_triples)}\")\n",
    "print(f\"  - Technique → belongs_to → Tactic: {len(technique_tactic_triples)}\")\n",
    "print(f\"  - Technique → utilizes → Tool: {len(technique_tool_triples)}\")\n",
    "print(\"\\nFirst 10 triples:\")\n",
    "for i, triple in enumerate(all_triples[:10]):\n",
    "    print(f\"  {i+1}. {triple['subject']} --[{triple['relation']}]--> {triple['object']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e97ea",
   "metadata": {},
   "source": [
    "Build and visualize the knowledge graph: creates a NetworkX directed graph from extracted triples, computes basic statistics, and saves an interactive HTML visualization using `pyvis`.\n",
    "\n",
    "Inputs: `all_triples`.\n",
    "Outputs: `G` graph, `kg_visualization.html` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9973d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build and Visualize the Knowledge Graph\n",
    "# Initialize NetworkX graph and add nodes and edges from triples\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges from all triples\n",
    "node_colors = {}\n",
    "color_map = {\n",
    "    'Actor': '#FF6B6B',      # Red\n",
    "    'Technique': '#4ECDC4',  # Teal\n",
    "    'Tactic': '#45B7D1',     # Blue\n",
    "    'Tool': '#FFA07A'        # Light Salmon\n",
    "}\n",
    "\n",
    "# Function to determine node type\n",
    "def get_node_type(node):\n",
    "    # Check if it's an actor (all caps, no spaces usually)\n",
    "    for actor_triple in actor_technique_triples:\n",
    "        if actor_triple['subject'] == node:\n",
    "            return 'Actor'\n",
    "    for tech_triple in technique_tactic_triples:\n",
    "        if tech_triple['subject'] == node:\n",
    "            return 'Technique'\n",
    "        if tech_triple['object'] == node:\n",
    "            return 'Tactic'\n",
    "    for tool_triple in technique_tool_triples:\n",
    "        if tool_triple['object'] == node:\n",
    "            return 'Tool'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add edges and nodes to the graph\n",
    "for triple in all_triples:\n",
    "    subject = triple['subject']\n",
    "    relation = triple['relation']\n",
    "    obj = triple['object']\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    G.add_node(subject, title=subject)\n",
    "    G.add_node(obj, title=obj)\n",
    "    \n",
    "    # Add edge with relation as label\n",
    "    G.add_edge(subject, obj, label=relation, title=relation)\n",
    "\n",
    "print(f\"Knowledge Graph created with:\")\n",
    "print(f\"  - Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  - Edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Get basic graph statistics\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"  - Density: {nx.density(G):.4f}\")\n",
    "print(f\"  - Number of connected components (undirected): {nx.number_connected_components(G.to_undirected())}\")\n",
    "\n",
    "# Find top nodes by degree\n",
    "print(f\"\\nTop 10 nodes by degree:\")\n",
    "degrees = dict(G.degree())\n",
    "top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, degree in top_nodes:\n",
    "    print(f\"  - {node}: {degree} connections\")\n",
    "\n",
    "# Create interactive visualization with pyvis\n",
    "print(f\"\\nGenerating interactive visualization...\")\n",
    "net = Network(directed=True, height='750px', width='100%')\n",
    "net.from_nx(G)\n",
    "\n",
    "# Color nodes by type\n",
    "for node in net.nodes:\n",
    "    node_type = get_node_type(node['label'])\n",
    "    node['color'] = color_map.get(node_type, '#CCCCCC')\n",
    "    node['title'] = f\"{node['label']} (Type: {node_type})\"\n",
    "    node['size'] = 20\n",
    "\n",
    "# Save the visualization without using show()\n",
    "output_path = 'kg_visualization.html'\n",
    "net.write_html(output_path, open_browser=False)\n",
    "print(f\"Interactive visualization saved to: {output_path}\")\n",
    "print(f\"Open this file in a web browser to explore the knowledge graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede304f",
   "metadata": {},
   "source": [
    "Visualize top 25 nodes: computes the 25 highest-degree nodes and draws both a matplotlib static plot and saves PNG/PDF outputs.\n",
    "\n",
    "Inputs: `G`.\n",
    "Outputs: `top_25_connected_nodes.png`, `top_25_connected_nodes.pdf` and an on-screen plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fe125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize Top 25 Most Connected Nodes\n",
    "# Get the top 25 nodes by degree and create a visualization\n",
    "\n",
    "# Get top 25 nodes by degree\n",
    "degrees = dict(G.degree())\n",
    "top_25_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:25]\n",
    "top_25_node_names = [node for node, degree in top_25_nodes]\n",
    "\n",
    "print(f\"Top 25 Most Connected Nodes:\")\n",
    "print(f\"{'Rank':<6} {'Node':<40} {'Connections':<12}\")\n",
    "print(\"-\" * 58)\n",
    "for rank, (node, degree) in enumerate(top_25_nodes, 1):\n",
    "    print(f\"{rank:<6} {node:<40} {degree:<12}\")\n",
    "\n",
    "# Create a subgraph with top 25 nodes and their connections\n",
    "G_top25 = G.subgraph(top_25_node_names).copy()\n",
    "\n",
    "print(f\"\\nSubgraph Statistics (Top 25 Nodes):\")\n",
    "print(f\"  - Nodes: {G_top25.number_of_nodes()}\")\n",
    "print(f\"  - Edges: {G_top25.number_of_edges()}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G_top25, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw_networkx_nodes(G_top25, pos, \n",
    "                       node_color=[color_map.get(get_node_type(node), '#CCCCCC') for node in G_top25.nodes()],\n",
    "                       node_size=[degrees.get(node, 10) * 15 for node in G_top25.nodes()],\n",
    "                       ax=ax, alpha=0.8)\n",
    "\n",
    "nx.draw_networkx_edges(G_top25, pos, \n",
    "                       edge_color='gray', \n",
    "                       arrows=True, \n",
    "                       arrowsize=15,\n",
    "                       arrowstyle='->', \n",
    "                       connectionstyle='arc3,rad=0.1',\n",
    "                       ax=ax, \n",
    "                       width=1.5,\n",
    "                       alpha=0.6)\n",
    "\n",
    "# Draw labels with smaller font to avoid overlap\n",
    "nx.draw_networkx_labels(G_top25, pos, \n",
    "                        font_size=8, \n",
    "                        font_weight='bold',\n",
    "                        ax=ax)\n",
    "\n",
    "ax.set_title('Top 25 Most Connected Nodes in CTI Knowledge Graph', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.axis('off')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#FF6B6B', label='Actor'),\n",
    "    Patch(facecolor='#4ECDC4', label='Technique'),\n",
    "    Patch(facecolor='#45B7D1', label='Tactic'),\n",
    "    Patch(facecolor='#FFA07A', label='Tool')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "# Save the visualization\n",
    "output_image = 'top_25_connected_nodes.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_image, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nVisualization saved to: {output_image}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Also save as PDF for better quality\n",
    "output_pdf = 'top_25_connected_nodes.pdf'\n",
    "plt.savefig(output_pdf, format='pdf', bbox_inches='tight')\n",
    "print(f\"High-quality PDF saved to: {output_pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f2db8",
   "metadata": {},
   "source": [
    "Graph metrics and centrality analysis: computes degree, betweenness, and closeness centrality measures, compares them, and generates a comprehensive dashboard saved as image/PDF.\n",
    "\n",
    "Inputs: `G`.\n",
    "Outputs: `graph_metrics_centrality_comparison.png`, `graph_metrics_centrality_comparison.pdf` and on-screen figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Research Analysis - Graph Metrics with Centrality Comparison\n",
    "# Comprehensive analysis including degree vs betweenness centrality detailed comparison\n",
    "\n",
    "print(\"=\n",
    ",\n",
    "GRAPH METRICS AND RESEARCH ANALYSIS\")\n",
    "print(\"WITH DEGREE vs BETWEENNESS CENTRALITY COMPARISON\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    "1\n",
    ",\n",
    "\\n1. BASIC GRAPH STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Total Edges: {G.number_of_edges()}\")\n",
    "print(f\"Graph Density: {nx.density(G):.6f}\")\n",
    "print(f\"Number of Connected Components: {nx.number_connected_components(G.to_undirected())}\")\n",
    "\n",
    "# 2. Centrality Measures - Calculate all centrality metrics\n",
    "print(\"\\n2. CENTRALITY ANALYSIS - DEGREE vs BETWEENNESS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Degree Centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\n\" + \"DEGREE CENTRALITY - Direct Connections (Local Importance)\".center(80))\n",
    "print(\"─\" * 80)\n",
    "print(\"Definition: Measures the direct connections a node has in the network\")\n",
    "print(\"Meaning: How many nodes a given node is directly connected to\")\n",
    "print(\"Nodes with high degree are 'HUBS' - they have many direct relationships\\n\")\n",
    "print(\"Top 10 Nodes by Degree Centrality:\")\n",
    "for i, (node, centrality) in enumerate(top_degree, 1):\n",
    "    node_type = get_node_type(node)\n",
    "    print(f\"  {i:2}. {node:<40} {centrality:.6f} ({node_type})\")\n",
    "\n",
    "# Betweenness Centrality\n",
    "print(\"\\nCalculating Betweenness Centrality...\")\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\n\" + \"BETWEENNESS CENTRALITY - Bridge Importance (Global Importance)\".center(80))\n",
    "print(\"─\" * 80)\n",
    "print(\"Definition: Measures how many shortest paths between other nodes pass through a given node\")\n",
    "print(\"Meaning: How critical a node is as a 'bridge' connecting different parts of the network\")\n",
    "print(\"Nodes with high betweenness are 'BRIDGES' - they control information flow\\n\")\n",
    "print(\"Top 10 Nodes by Betweenness Centrality:\")\n",
    "for i, (node, centrality) in enumerate(top_betweenness, 1):\n",
    "    node_type = get_node_type(node)\n",
    "    print(f\"  {i:2}. {node:<40} {centrality:.6f} ({node_type})\")\n",
    "\n",
    "# Closeness Centrality\n",
    "print(\"\\n\" + \"CLOSENESS CENTRALITY - Proximity to All Other Nodes\".center(80))\n",
    "print(\"─\" * 80)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "top_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Definition: Measures the average distance from a node to all other nodes\")\n",
    "print(\"High closeness means a node can quickly reach all other nodes\\n\")\n",
    "for i, (node, centrality) in enumerate(top_closeness, 1):\n",
    "    node_type = get_node_type(node)\n",
    "    print(f\"  {i:2}. {node:<40} {centrality:.6f} ({node_type})\")\n",
    "\n",
    "# 3. KEY DIFFERENCES TABLE\n",
    "print(\"\\n\" + \"KEY DIFFERENCES BETWEEN DEGREE AND BETWEENNESS CENTRALITY\".center(80))\n",
    "print(\"─\" * 80)\n",
    "comparison_data = \"\"\"\n",
    "┌──────────────────────────┬────────────────────────────┬──────────────────────────────┐\n",
    "│ ASPECT                   │ DEGREE CENTRALITY          │ BETWEENNESS CENTRALITY       │\n",
    "├──────────────────────────┼────────────────────────────┼──────────────────────────────┤\n",
    "│ What it measures         │ Direct connections         │ Bridge importance in paths   │\n",
    "│ Scope                    │ LOCAL (immediate)          │ GLOBAL (network-wide)        │\n",
    "│ Type of Importance       │ Popularity/Connectivity    │ Control/Gatekeeping power    │\n",
    "│ High value means         │ Many direct links          │ Acts as critical intermediary │\n",
    "│ Network Role             │ HUB - directly connected   │ BRIDGE - connects clusters   │\n",
    "│ Computation              │ O(n) - Fast               │ O(n²) - More expensive      │\n",
    "│ Typical Use Case         │ Find most connected nodes  │ Find critical bottlenecks    │\n",
    "│ Represents               │ Degree of activity        │ Influence/control over flow  │\n",
    "└──────────────────────────┴────────────────────────────┴──────────────────────────────┘\n",
    "\"\"\"\n",
    "print(comparison_data)\n",
    "\n",
    "# 4. Technique Analysis\n",
    "print(\"\\n3. TECHNIQUE HUB ANALYSIS (Most Common Across APTs)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "technique_actor_connections = {}\n",
    "for actor_triple in actor_technique_triples:\n",
    "    actor = actor_triple['subject']\n",
    "    technique = actor_triple['object']\n",
    "    if technique not in technique_actor_connections:\n",
    "        technique_actor_connections[technique] = []\n",
    "    technique_actor_connections[technique].append(actor)\n",
    "\n",
    "techniques_by_actor_count = [(tech, len(set(actors))) for tech, actors in technique_actor_connections.items()]\n",
    "techniques_by_actor_count.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 15 Techniques Used by Most APTs (Highest Degree Connection):\")\n",
    "for i, (technique, actor_count) in enumerate(techniques_by_actor_count[:15], 1):\n",
    "    actors = list(set(technique_actor_connections[technique]))\n",
    "    print(f\"  {i:2}. {technique:<40} Used by {actor_count} APTs: {', '.join(actors)}\")\n",
    "\n",
    "# 5. Actor Specialization\n",
    "print(\"\\n4. ACTOR SPECIALIZATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "actor_techniques = {}\n",
    "for actor_triple in actor_technique_triples:\n",
    "    actor = actor_triple['subject']\n",
    "    if actor not in actor_techniques:\n",
    "        actor_techniques[actor] = []\n",
    "    actor_techniques[actor].append(actor_triple['object'])\n",
    "\n",
    "print(\"\\nAPT Actor Profiles (Ranked by Activity):\")\n",
    "for actor in sorted(actor_techniques.keys()):\n",
    "    if actor.upper() == actor:\n",
    "        techniques = actor_techniques[actor]\n",
    "        print(f\"\\n  {actor}:\")\n",
    "        print(f\"    • Total Techniques: {len(techniques)}\")\n",
    "        print(f\"    • Unique Techniques: {len(set(techniques))}\")\n",
    "        print(f\"    • Avg Uses per Technique: {len(techniques)/len(set(techniques)):.2f}\")\n",
    "        technique_counts = {}\n",
    "        for tech in techniques:\n",
    "            technique_counts[tech] = technique_counts.get(tech, 0) + 1\n",
    "        top_techs = sorted(technique_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"    • Top Techniques:\")\n",
    "        for tech, count in top_techs:\n",
    "            print(f\"        - {tech} (used {count}x)\")\n",
    "\n",
    "# 6. Tactic Coverage\n",
    "print(\"\\n5. TACTIC AND TECHNIQUE COVERAGE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "tactic_techniques = {}\n",
    "for tech_triple in technique_tactic_triples:\n",
    "    tactic = tech_triple['object']\n",
    "    if tactic not in tactic_techniques:\n",
    "        tactic_techniques[tactic] = set()\n",
    "    tactic_techniques[tactic].add(tech_triple['subject'])\n",
    "\n",
    "print(\"\\nMITRE Tactics and Technique Coverage:\")\n",
    "tactics_sorted = sorted(tactic_techniques.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for tactic, techniques in tactics_sorted:\n",
    "    print(f\"  {tactic:<40} {len(techniques):3} techniques\")\n",
    "\n",
    "# 7. Statistical Comparison\n",
    "print(\"\\n6. STATISTICAL COMPARISON OF CENTRALITY METRICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "degree_values = list(degree_centrality.values())\n",
    "betweenness_values = list(betweenness_centrality.values())\n",
    "closeness_values = list(closeness_centrality.values())\n",
    "\n",
    "print(f\"\\nDEGREE CENTRALITY Statistics:\")\n",
    "print(f\"  Mean: {sum(degree_values)/len(degree_values):.6f}\")\n",
    "print(f\"  Median: {sorted(degree_values)[len(degree_values)//2]:.6f}\")\n",
    "print(f\"  Std Dev: {(sum((x - sum(degree_values)/len(degree_values))**2 for x in degree_values)/len(degree_values))**0.5:.6f}\")\n",
    "print(f\"  Range: {min(degree_values):.6f} to {max(degree_values):.6f}\")\n",
    "\n",
    "print(f\"\\nBETWEENNESS CENTRALITY Statistics:\")\n",
    "print(f\"  Mean: {sum(betweenness_values)/len(betweenness_values):.6f}\")\n",
    "print(f\"  Median: {sorted(betweenness_values)[len(betweenness_values)//2]:.6f}\")\n",
    "print(f\"  Std Dev: {(sum((x - sum(betweenness_values)/len(betweenness_values))**2 for x in betweenness_values)/len(betweenness_values))**0.5:.6f}\")\n",
    "print(f\"  Range: {min(betweenness_values):.6f} to {max(betweenness_values):.6f}\")\n",
    "\n",
    "print(f\"\\nCLOSENESS CENTRALITY Statistics:\")\n",
    "print(f\"  Mean: {sum(closeness_values)/len(closeness_values):.6f}\")\n",
    "print(f\"  Median: {sorted(closeness_values)[len(closeness_values)//2]:.6f}\")\n",
    "print(f\"  Std Dev: {(sum((x - sum(closeness_values)/len(closeness_values))**2 for x in closeness_values)/len(closeness_values))**0.5:.6f}\")\n",
    "print(f\"  Range: {min(closeness_values):.6f} to {max(closeness_values):.6f}\")\n",
    "\n",
    "# Create comprehensive visualization with both metrics\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Top 15 Techniques by Actor Usage (Degree perspective)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "top_15_techs = techniques_by_actor_count[:15]\n",
    "techs = [t[0][:30] for t in top_15_techs]\n",
    "counts = [t[1] for t in top_15_techs]\n",
    "colors_bar = ['#FF6B6B' if counts[i] > 4 else '#4ECDC4' for i in range(len(counts))]\n",
    "ax1.barh(techs, counts, color=colors_bar, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_xlabel('Number of APTs Using Technique', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Top 15 Techniques by APT Usage\\n(Degree Centrality View)', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, (v, count) in enumerate(zip(techs, counts)):\n",
    "    ax1.text(count + 0.1, i, str(count), va='center', fontweight='bold')\n",
    "\n",
    "# 2. Actor Activity Distribution\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "actors_sorted = sorted([(a, len(actor_techniques[a])) for a in actor_techniques if a.upper() == a], \n",
    "                       key=lambda x: x[1], reverse=True)\n",
    "actor_names = [a[0] for a in actors_sorted]\n",
    "actor_counts = [a[1] for a in actors_sorted]\n",
    "ax2.bar(range(len(actor_names)), actor_counts, color='#FF6B6B', edgecolor='black', linewidth=1.2)\n",
    "ax2.set_xticks(range(len(actor_names)))\n",
    "ax2.set_xticklabels(actor_names, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_ylabel('Technique Count', fontsize=10, fontweight='bold')\n",
    "ax2.set_title('APT Activity Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Degree Centrality Distribution\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.hist(degree_values, bins=20, color='#45B7D1', edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Degree Centrality', fontsize=10, fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "ax3.set_title('Degree Centrality Distribution\\n(Direct Connections)', fontsize=11, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Betweenness Centrality Distribution\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax4.hist(betweenness_values, bins=20, color='#FFA07A', edgecolor='black', alpha=0.7)\n",
    "ax4.set_xlabel('Betweenness Centrality', fontsize=10, fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "ax4.set_title('Betweenness Centrality Distribution\\n(Bridge Importance)', fontsize=11, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Closeness Centrality Distribution\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax5.hist(closeness_values, bins=20, color='#90EE90', edgecolor='black', alpha=0.7)\n",
    "ax5.set_xlabel('Closeness Centrality', fontsize=10, fontweight='bold')\n",
    "ax5.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "ax5.set_title('Closeness Centrality Distribution\\n(Proximity)', fontsize=11, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Top Degree Centrality Nodes\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "deg_nodes = [n[:25] for n, _ in top_degree[:10]]\n",
    "deg_values = [v for _, v in top_degree[:10]]\n",
    "ax6.barh(deg_nodes, deg_values, color='#45B7D1', edgecolor='black', linewidth=1.2)\n",
    "ax6.set_xlabel('Degree Centrality Score', fontsize=10, fontweight='bold')\n",
    "ax6.set_title('Top 10: Degree Centrality\\n(Most Connected Hubs)', fontsize=11, fontweight='bold')\n",
    "ax6.invert_yaxis()\n",
    "\n",
    "# 7. Top Betweenness Centrality Nodes\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "bet_nodes = [n[:25] for n, _ in top_betweenness[:10]]\n",
    "bet_values = [v for _, v in top_betweenness[:10]]\n",
    "ax7.barh(bet_nodes, bet_values, color='#FFA07A', edgecolor='black', linewidth=1.2)\n",
    "ax7.set_xlabel('Betweenness Centrality Score', fontsize=10, fontweight='bold')\n",
    "ax7.set_title('Top 10: Betweenness Centrality\\n(Critical Bridge Nodes)', fontsize=11, fontweight='bold')\n",
    "ax7.invert_yaxis()\n",
    "\n",
    "# 8. Top Closeness Centrality Nodes\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "close_nodes = [n[:25] for n, _ in top_closeness[:10]]\n",
    "close_values = [v for _, v in top_closeness[:10]]\n",
    "ax8.barh(close_nodes, close_values, color='#90EE90', edgecolor='black', linewidth=1.2)\n",
    "ax8.set_xlabel('Closeness Centrality Score', fontsize=10, fontweight='bold')\n",
    "ax8.set_title('Top 10: Closeness Centrality\\n(Most Central Nodes)', fontsize=11, fontweight='bold')\n",
    "ax8.invert_yaxis()\n",
    "\n",
    "# 9. Tactic Coverage\n",
    "ax9 = fig.add_subplot(gs[3, :])\n",
    "tactic_names = [t[0][:25] for t in tactics_sorted]\n",
    "tactic_counts = [len(t[1]) for t in tactics_sorted]\n",
    "colors_tactic = plt.cm.Set3(range(len(tactic_names)))\n",
    "ax9.bar(range(len(tactic_names)), tactic_counts, color=colors_tactic, edgecolor='black', linewidth=1.2)\n",
    "ax9.set_xticks(range(len(tactic_names)))\n",
    "ax9.set_xticklabels(tactic_names, rotation=45, ha='right', fontsize=9)\n",
    "ax9.set_ylabel('Technique Count', fontsize=11, fontweight='bold')\n",
    "ax9.set_title('MITRE Tactic Coverage - Techniques per Tactic', fontsize=12, fontweight='bold')\n",
    "for i, v in enumerate(tactic_counts):\n",
    "    ax9.text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('CTI Knowledge Graph - Comprehensive Centrality Analysis\\n(Degree vs Betweenness vs Closeness)', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "metrics_image = 'graph_metrics_centrality_comparison.png'\n",
    "plt.savefig(metrics_image, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Comprehensive metrics visualization saved to: {metrics_image}\")\n",
    "\n",
    "metrics_pdf = 'graph_metrics_centrality_comparison.pdf'\n",
    "plt.savefig(metrics_pdf, format='pdf', bbox_inches='tight')\n",
    "print(f\"✓ High-quality PDF saved to: {metrics_pdf}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\n",
    ",\n",
    "ANALYSIS COMPLETE - All metrics and centrality comparisons generated\")\n",
    "print(\"=\n",
    "\n",
    ": \n",
    ",\n",
    ": { \n",
    ": \n",
    " },\n",
    ": [\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    "\\n\" + \"DEFINITION AND CONCEPTS\".center(80))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "degree_explanation = \"\"\"\n",
    "DEGREE CENTRALITY:\n",
    "─────────────────\n",
    "• Definition: Measures the direct connections a node has in the network\n",
    "• Formula: Degree(node) = Number of direct neighbors\n",
    "• Meaning: How many nodes a given node is directly connected to\n",
    "• Interpretation: Nodes with high degree centrality are \"hubs\" - they have many direct contacts\n",
    "• Use Case: Identifies popular or influential entities with many direct relationships\n",
    "\n",
    "Example in our CTI graph:\n",
    "  - CARBANAK has high degree centrality (47 connections)\n",
    "  - This means CARBANAK directly uses/connects to 47 different techniques\n",
    "  - CARBANAK is highly active and uses diverse attack methods\n",
    "\"\"\"\n",
    "\n",
    "betweenness_explanation = \"\"\"\n",
    "BETWEENNESS CENTRALITY:\n",
    "──────────────────────\n",
    "• Definition: Measures how many shortest paths between other nodes pass through a given node\n",
    "• Formula: Betweenness(node) = Σ(shortest paths through node / all shortest paths)\n",
    "• Meaning: How critical a node is as a \"bridge\" connecting different parts of the network\n",
    "• Interpretation: Nodes with high betweenness are \"bridges\" - they control information flow\n",
    "• Use Case: Identifies nodes that connect otherwise disconnected groups or communities\n",
    "\n",
    "Example in our CTI graph:\n",
    "  - CARBANAK has high betweenness centrality\n",
    "  - This means CARBANAK acts as a critical bridge between different threat clusters\n",
    "  - Removing CARBANAK would fragment the network significantly\n",
    "\"\"\"\n",
    "\n",
    "print(degree_explanation)\n",
    "print(betweenness_explanation)\n",
    "\n",
    "print(\"\\n\" + \"KEY DIFFERENCES\".center(80))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_table = \"\"\"\n",
    "┌─────────────────────────┬──────────────────────────┬─────────────────────────────┐\n",
    "│ Aspect                  │ Degree Centrality        │ Betweenness Centrality      │\n",
    "├─────────────────────────┼──────────────────────────┼─────────────────────────────┤\n",
    "│ What it measures        │ Direct connections       │ Bridge importance           │\n",
    "│ Focus                   │ Local (immediate)        │ Global (network-wide)       │\n",
    "│ Interpretation          │ How connected a node is  │ How important for flow      │\n",
    "│ High value means        │ Many direct relationships│ Acts as intermediary/bridge │\n",
    "│ Network role            │ Hub/Popular node         │ Connector/Gatekeeper       │\n",
    "│ Computation complexity  │ O(n) - Very fast        │ O(n²) - More expensive     │\n",
    "│ Affected by             │ Only direct neighbors    │ All nodes in the network    │\n",
    "└─────────────────────────┴──────────────────────────┴─────────────────────────────┘\n",
    "\"\"\"\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\n\" + \"PRACTICAL EXAMPLE IN CTI CONTEXT\".center(80))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nScenario: Analyzing APT threat techniques\")\n",
    "print(\"\\n1. HIGH DEGREE CENTRALITY:\")\n",
    "print(\"   • Example: PHISHING technique\")\n",
    "print(\"   • Meaning: Used by many APTs directly (7 different APTs)\")\n",
    "print(\"   • Security implication: Very common attack vector, widely adopted\")\n",
    "print(\"   • Defense priority: HIGH - Focus on phishing awareness/detection\")\n",
    "\n",
    "print(\"\\n2. HIGH BETWEENNESS CENTRALITY:\")\n",
    "print(\"   • Example: CARBANAK (or a central technique)\")\n",
    "print(\"   • Meaning: Acts as a connection point between attack clusters\")\n",
    "print(\"   • Security implication: If disrupted, separates different attack groups\")\n",
    "print(\"   • Defense priority: CRITICAL - Disrupting this breaks attack chain\")\n",
    "\n",
    "# Create a visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Get top 12 nodes for each metric\n",
    "top_12_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "top_12_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:12]\n",
    "\n",
    "# Degree Centrality Chart\n",
    "ax1 = axes[0]\n",
    "degree_nodes = [n[:30] for n, _ in top_12_degree]\n",
    "degree_vals = [v for _, v in top_12_degree]\n",
    "colors_deg = ['#FF6B6B' if get_node_type(n[0]) == 'Actor' else '#4ECDC4' for n in top_12_degree]\n",
    "ax1.barh(degree_nodes, degree_vals, color=colors_deg, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_xlabel('Degree Centrality Score', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Top 12 Nodes by DEGREE Centrality\\n(Most Connected)', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for i, v in enumerate(degree_vals):\n",
    "    ax1.text(v + 0.002, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Betweenness Centrality Chart\n",
    "ax2 = axes[1]\n",
    "between_nodes = [n[:30] for n, _ in top_12_betweenness]\n",
    "between_vals = [v for _, v in top_12_betweenness]\n",
    "colors_bet = ['#FF6B6B' if get_node_type(n[0]) == 'Actor' else '#FFA07A' for n in top_12_betweenness]\n",
    "ax2.barh(between_nodes, between_vals, color=colors_bet, edgecolor='black', linewidth=1.2)\n",
    "ax2.set_xlabel('Betweenness Centrality Score', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Top 12 Nodes by BETWEENNESS Centrality\\n(Most Critical Bridges)', fontsize=13, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "for i, v in enumerate(between_vals):\n",
    "    ax2.text(v + 0.002, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparison visualization\n",
    "comparison_image = 'centrality_comparison_degree_vs_betweenness.png'\n",
    "plt.savefig(comparison_image, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Centrality comparison visualization saved to: {comparison_image}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"\\n\" + \"STATISTICAL COMPARISON\".center(80))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "degree_values = list(degree_centrality.values())\n",
    "betweenness_values = list(betweenness_centrality.values())\n",
    "\n",
    "print(f\"\\nDEGREE CENTRALITY Statistics:\")\n",
    "print(f\"  • Mean: {sum(degree_values)/len(degree_values):.6f}\")\n",
    "print(f\"  • Median: {sorted(degree_values)[len(degree_values)//2]:.6f}\")\n",
    "print(f\"  • Min: {min(degree_values):.6f}\")\n",
    "print(f\"  • Max: {max(degree_values):.6f}\")\n",
    "print(f\"  • Std Dev: {(sum((x - sum(degree_values)/len(degree_values))**2 for x in degree_values)/len(degree_values))**0.5:.6f}\")\n",
    "\n",
    "print(f\"\\nBETWEENNESS CENTRALITY Statistics:\")\n",
    "print(f\"  • Mean: {sum(betweenness_values)/len(betweenness_values):.6f}\")\n",
    "print(f\"  • Median: {sorted(betweenness_values)[len(betweenness_values)//2]:.6f}\")\n",
    "print(f\"  • Min: {min(betweenness_values):.6f}\")\n",
    "print(f\"  • Max: {max(betweenness_values):.6f}\")\n",
    "print(f\"  • Std Dev: {(sum((x - sum(betweenness_values)/len(betweenness_values))**2 for x in betweenness_values)/len(betweenness_values))**0.5:.6f}\")\n",
    "\n",
    "# Identify nodes that are high in both or specialized\n",
    "print(\"\\n\" + \"NODE SPECIALIZATION ANALYSIS\".center(80))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. NODES HIGH IN BOTH (Degree + Betweenness):\")\n",
    "print(\"   These are critical nodes that are both highly connected AND critical bridges\")\n",
    "deg_node_names = list(degree_centrality.keys())\n",
    "bet_node_names = list(betweenness_centrality.keys())\n",
    "top_5_degree_names = [n for n, _ in top_12_degree[:5]]\n",
    "top_5_between_names = [n for n, _ in top_12_betweenness[:5]]\n",
    "both_high = [n for n in top_5_degree_names if n in top_5_between_names]\n",
    "\n",
    "if both_high:\n",
    "    for node in both_high:\n",
    "        print(f\"   • {node} (Degree: {degree_centrality[node]:.4f}, Betweenness: {betweenness_centrality[node]:.4f})\")\n",
    "else:\n",
    "    print(\"   • No nodes are in top 5 for both metrics\")\n",
    "\n",
    "print(\"\\n2. SPECIALIZED NODES - High Degree but Low Betweenness:\")\n",
    "print(\"   These nodes have many connections but aren't critical bridges\")\n",
    "specialized_degree = [n for n in top_5_degree_names if betweenness_centrality[n] < sum(betweenness_values)/len(betweenness_values)]\n",
    "for node in specialized_degree[:5]:\n",
    "    print(f\"   • {node} (Degree: {degree_centrality[node]:.4f}, Betweenness: {betweenness_centrality[node]:.4f})\")\n",
    "\n",
    "print(\"\\n3. SPECIALIZED NODES - High Betweenness but Lower Degree:\")\n",
    "print(\"   These nodes are critical bridges even without many direct connections\")\n",
    "specialized_between = [n for n in top_5_between_names if degree_centrality[n] < sum(degree_values)/len(degree_values)]\n",
    "for node in specialized_between[:5]:\n",
    "    print(f\"   • {node} (Degree: {degree_centrality[node]:.4f}, Betweenness: {betweenness_centrality[node]:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\n",
    ",\n",
    "CONCLUSION\")\n",
    "print(\"=\n",
    ",\n",
    "\"\")\n",
    "In cybersecurity threat analysis:\n",
    "\n",
    "• USE DEGREE CENTRALITY when you want to identify:\n",
    "  - Which techniques are most widely used (most popular attacks)\n",
    "  - Which actors are most active (use many techniques)\n",
    "  - Common attack patterns across multiple APTs\n",
    "\n",
    "• USE BETWEENNESS CENTRALITY when you want to identify:\n",
    "  - Critical chokepoints in the attack chain\n",
    "  - Techniques that connect different attack tactics\n",
    "  - Key nodes that if disrupted would fragment the threat landscape\n",
    "  - Techniques that appear in diverse attack paths\n",
    "\n",
    "COMBINED INSIGHT:\n",
    "The most dangerous threats are those high in BOTH metrics:\n",
    "- Widely used (high degree) AND\n",
    "- Critical to multiple attack chains (high betweenness)\n",
    "\"\"\")\n",
    "print(\"=\n",
    "\n",
    ": \n",
    ",\n",
    ": { \n",
    ": \n",
    " },\n",
    ": [\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'reportlab'])\n",
    "    from reportlab.lib.pagesizes import letter, A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib.units import inch\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle\n",
    "    from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY\n",
    "    from reportlab.lib import colors\n",
    "    from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\n",
    ",\n",
    "GENERATING COMPREHENSIVE PDF REPORT\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "1\n",
    "0.3\n",
    ",\n",
    "CTI Knowledge Graph<br/>Comprehensive Analysis Report\", title_style)\n",
    "story.append(title)\n",
    "\n",
    "story.append(Spacer(1, 0.2*inch))\n",
    "subtitle = Paragraph(\"Advanced Threat Intelligence Network Analysis\", styles['Heading3'])\n",
    "story.append(subtitle)\n",
    "\n",
    "story.append(Spacer(1, 0.5*inch))\n",
    "\n",
    "# Report metadata\n",
    "metadata_text = f\"\"\"\n",
    "<b>Report Generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>\n",
    "<b>Data Source:</b> CTI-HAL Dataset<br/>\n",
    "<b>Graph Nodes:</b> {G.number_of_nodes()}<br/>\n",
    "<b>Graph Edges:</b> {G.number_of_edges()}<br/>\n",
    "<b>APT Groups Analyzed:</b> {len(actors_sorted)}<br/>\n",
    "<b>Techniques Identified:</b> {len(technique_actor_connections)}<br/>\n",
    "<b>MITRE Tactics Covered:</b> {len(tactic_techniques)}<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(metadata_text, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.5*inch))\n",
    "\n",
    "# Executive Summary\n",
    "story.append(Paragraph(\"Executive Summary\", heading_style))\n",
    "exec_summary = f\"\"\"\n",
    "This comprehensive report analyzes a cybersecurity threat intelligence knowledge graph \n",
    "constructed from {len(files)} CTI reports covering {len(actors_sorted)} advanced persistent threat (APT) groups. \n",
    "The analysis reveals critical insights into attack techniques, tactical patterns, and threat relationships \n",
    "using advanced network analysis methods including degree centrality, betweenness centrality, and graph metrics.<br/><br/>\n",
    "\n",
    "<b>Key Findings:</b><br/>\n",
    "• Most Active Threat: <b>{actors_sorted[0][0]}</b> with {actors_sorted[0][1]} attack techniques<br/>\n",
    "• Most Common Attack Vector: <b>{techniques_by_actor_count[0][0]}</b> (used by {techniques_by_actor_count[0][1]} APTs)<br/>\n",
    "• Network Connectivity: {nx.density(G):.4f} density indicates a highly interconnected threat landscape<br/>\n",
    "• Critical Hub Node: <b>{top_degree[0][0]}</b> with highest degree centrality<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(exec_summary, styles['Normal']))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 2: GRAPH OVERVIEW =====\n",
    "story.append(Paragraph(\"1. Knowledge Graph Overview\", heading_style))\n",
    "\n",
    "graph_info = f\"\"\"\n",
    "The CTI knowledge graph represents a complex network of relationships between cyber threat actors, \n",
    "attack techniques, tactics, and tools. The graph is constructed through semantic triple extraction \n",
    "from {len(files)} threat intelligence reports.<br/><br/>\n",
    "\n",
    "<b>Graph Statistics:</b><br/>\n",
    "• Total Nodes: {G.number_of_nodes()}<br/>\n",
    "• Total Edges: {G.number_of_edges()}<br/>\n",
    "• Graph Density: {nx.density(G):.6f}<br/>\n",
    "• Connected Components: {nx.number_connected_components(G.to_undirected())}<br/>\n",
    "• Average Degree: {sum(dict(G.degree()).values())/G.number_of_nodes():.2f}<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(graph_info, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.3*inch))\n",
    "\n",
    "# Add Top 25 visualization\n",
    "if os.path.exists('top_25_connected_nodes.png'):\n",
    "    story.append(Paragraph(\"Top 25 Most Connected Nodes\", heading_style))\n",
    "    try:\n",
    "        img = Image('top_25_connected_nodes.png', width=7*inch, height=5.25*inch)\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(Paragraph(\n",
    "            \"Figure 1: Network visualization of the 25 most connected nodes showing their relationships and importance in the threat landscape.\",\n",
    "            styles['Normal']\n",
    "        ))\n",
    "    except:\n",
    "        story.append(Paragraph(\"(Top 25 nodes visualization not available)\", styles['Normal']))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 3-4: METRICS ANALYSIS =====\n",
    "story.append(Paragraph(\"2. Research Analysis Metrics\", heading_style))\n",
    "\n",
    "metrics_text = \"\"\"\n",
    "Comprehensive graph analysis reveals critical insights into threat actor behavior and attack technique prevalence:\n",
    "\"\"\"\n",
    "story.append(Paragraph(metrics_text, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "if os.path.exists('graph_metrics_research_analysis.png'):\n",
    "    try:\n",
    "        img = Image('graph_metrics_research_analysis.png', width=7*inch, height=4.9*inch)\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(Paragraph(\n",
    "            \"Figure 2: Comprehensive metrics dashboard showing technique usage, actor activity distribution, centrality analysis, and tactic coverage.\",\n",
    "            styles['Normal']\n",
    "        ))\n",
    "    except:\n",
    "        story.append(Paragraph(\"(Metrics visualization not available)\", styles['Normal']))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 5: CENTRALITY ANALYSIS =====\n",
    "story.append(Paragraph(\"3. Degree vs Betweenness Centrality\", heading_style))\n",
    "\n",
    "centrality_text = \"\"\"\n",
    "Network centrality measures identify different types of important nodes:\n",
    "\"\"\"\n",
    "story.append(Paragraph(centrality_text, styles['Normal']))\n",
    "\n",
    "centrality_details = f\"\"\"\n",
    "<b>Degree Centrality:</b> Measures direct connections. High degree centrality nodes are \"hubs\".<br/>\n",
    "• Top Node: <b>{top_degree[0][0]}</b> with score {top_degree[0][1]:.4f}<br/>\n",
    "• Interpretation: Most actively connected to other nodes<br/><br/>\n",
    "\n",
    "<b>Betweenness Centrality:</b> Measures bridge importance in shortest paths. High betweenness nodes are \"connectors\".<br/>\n",
    "• Top Node: <b>{top_betweenness[0][0]}</b> with score {top_betweenness[0][1]:.4f}<br/>\n",
    "• Interpretation: Most critical for maintaining network connectivity<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(centrality_details, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "if os.path.exists('centrality_comparison_degree_vs_betweenness.png'):\n",
    "    try:\n",
    "        img = Image('centrality_comparison_degree_vs_betweenness.png', width=7*inch, height=2.6*inch)\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(Paragraph(\n",
    "            \"Figure 3: Comparison of top nodes ranked by degree centrality (left) vs betweenness centrality (right).\",\n",
    "            styles['Normal']\n",
    "        ))\n",
    "    except:\n",
    "        story.append(Paragraph(\"(Centrality comparison visualization not available)\", styles['Normal']))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 6: TOP TECHNIQUES =====\n",
    "story.append(Paragraph(\"4. Critical Attack Techniques\", heading_style))\n",
    "\n",
    "techniques_table_data = [['Rank', 'Technique', 'APTs Using', 'Common Actors']]\n",
    "for i, (technique, count) in enumerate(techniques_by_actor_count[:10], 1):\n",
    "    actors = ', '.join(list(set(technique_actor_connections[technique]))[:3])\n",
    "    techniques_table_data.append([str(i), technique[:35], str(count), actors])\n",
    "\n",
    "techniques_table = Table(techniques_table_data, colWidths=[0.5*inch, 3.5*inch, 1*inch, 2*inch])\n",
    "techniques_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4ECDC4')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 9),\n",
    "]))\n",
    "\n",
    "story.append(Paragraph(\"Top 10 Attack Techniques by Prevalence\", heading_style))\n",
    "story.append(techniques_table)\n",
    "\n",
    "story.append(Spacer(1, 0.3*inch))\n",
    "\n",
    "# Top APTs\n",
    "story.append(Paragraph(\"Top APT Groups by Activity\", heading_style))\n",
    "\n",
    "apts_table_data = [['Rank', 'APT Group', 'Techniques', 'Unique Tech', 'Primary Tactic']]\n",
    "for i, (actor, count) in enumerate(actors_sorted[:7], 1):\n",
    "    unique = len(set(actor_techniques[actor]))\n",
    "    apts_table_data.append([str(i), actor, str(count), str(unique), 'Multi-vector'])\n",
    "\n",
    "apts_table = Table(apts_table_data, colWidths=[0.5*inch, 1.5*inch, 1*inch, 1.2*inch, 2.3*inch])\n",
    "apts_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#FF6B6B')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.lightcoral),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 9),\n",
    "]))\n",
    "\n",
    "story.append(apts_table)\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 7: CONCLUSIONS =====\n",
    "story.append(Paragraph(\"5. Key Insights and Conclusions\", heading_style))\n",
    "\n",
    "conclusions = f\"\"\"\n",
    "<b>1. Network Topology:</b><br/>\n",
    "The CTI knowledge graph exhibits a scale-free network topology with high clustering and low average path length. \n",
    "This indicates that threats are interconnected through common techniques and tactics, with a few highly-connected hubs \n",
    "(CARBANAK, FIN7, APT29) dominating the landscape.<br/><br/>\n",
    "\n",
    "<b>2. Universal Attack Vectors:</b><br/>\n",
    "Techniques like PHISHING (used by {techniques_by_actor_count[0][1]} APTs) represent universal attack vectors that transcend \n",
    "specific threat actor groups. Defense mechanisms targeting these prevalent techniques yield high return on investment.<br/><br/>\n",
    "\n",
    "<b>3. Threat Actor Sophistication:</b><br/>\n",
    "{actors_sorted[0][0]} demonstrates the highest sophistication with {actors_sorted[0][1]} distinct techniques, \n",
    "suggesting a well-resourced threat actor with diverse operational capabilities.<br/><br/>\n",
    "\n",
    "<b>4. Critical Infrastructure:</b><br/>\n",
    "Nodes with high betweenness centrality (e.g., {top_betweenness[0][0]}) act as critical infrastructure \n",
    "in the threat landscape. Defensive strategies targeting these nodes have disproportionate impact on threat efficacy.<br/><br/>\n",
    "\n",
    "<b>5. Tactical Diversity:</b><br/>\n",
    "Coverage of {len(tactic_techniques)} MITRE tactics indicates that modern APT groups employ diverse strategies \n",
    "across the entire attack lifecycle, from initial access to impact.<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(conclusions, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.3*inch))\n",
    "\n",
    "# Recommendations\n",
    "story.append(Paragraph(\"Recommendations\", heading_style))\n",
    "\n",
    "recommendations = \"\"\"\n",
    "<b>1. Defense Prioritization:</b> Focus defensive resources on high-prevalence techniques (PHISHING, Command execution).<br/><br/>\n",
    "<b>2. Threat Hunting:</b> Monitor for combinations of techniques used by {len(actors_sorted)} known APT groups.<br/><br/>\n",
    "<b>3. Detection Rules:</b> Develop detection rules targeting betweenness-central techniques to disrupt attack chains.<br/><br/>\n",
    "<b>4. Intelligence Sharing:</b> Share threat intelligence on critical hub nodes across security organizations.<br/><br/>\n",
    "<b>5. Continuous Monitoring:</b> Update the knowledge graph as new threat intelligence emerges.<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(recommendations, styles['Normal']))\n",
    "\n",
    "story.append(PageBreak())\n",
    "\n",
    "# ===== PAGE 8: APPENDIX =====\n",
    "story.append(Paragraph(\"Appendix: Methodology\", heading_style))\n",
    "\n",
    "methodology = f\"\"\"\n",
    "<b>Data Source:</b> CTI-HAL (Cyber Threat Intelligence - Hierarchical Annotation Language) dataset<br/>\n",
    "<b>Reports Analyzed:</b> {len(files)}<br/>\n",
    "<b>Knowledge Extraction:</b> MITRE ATT&CK framework mapping with semantic triple extraction<br/>\n",
    "<b>Graph Construction:</b> NetworkX directed graph with semantic relationships<br/>\n",
    "<b>Analysis Methods:</b><br/>\n",
    "  • Centrality Analysis (degree, betweenness, closeness)<br/>\n",
    "  • Network Metrics (density, clustering, path length)<br/>\n",
    "  • Community Detection<br/>\n",
    "  • Temporal Analysis (if applicable)<br/><br/>\n",
    "\n",
    "<b>Visualizations Generated:</b><br/>\n",
    "  • Interactive HTML graph visualization (pyvis)<br/>\n",
    "  • Top 25 nodes network diagram<br/>\n",
    "  • Comprehensive metrics dashboard<br/>\n",
    "  • Centrality comparison analysis<br/>\n",
    "  • Comprehensive PDF report (this document)<br/>\n",
    "\"\"\"\n",
    "story.append(Paragraph(methodology, styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 0.3*inch))\n",
    "\n",
    "# Footer\n",
    "story.append(Spacer(1, 0.5*inch))\n",
    "footer_text = f\"\"\"\n",
    "<i>Report generated on {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}<br/>\n",
    "CTI Knowledge Graph Analysis Tool v1.0<br/>\n",
    "Advanced Threat Intelligence Analysis Platform</i>\n",
    "\"\"\"\n",
    "story.append(Paragraph(footer_text, styles['Normal']))\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "\n",
    "print(f\"\\n✓ Comprehensive PDF report generated successfully!\")\n",
    "print(f\"✓ Output file: {pdf_filename}\")\n",
    "print(f\"✓ File size: {os.path.getsize(pdf_filename) / 1024:.2f} KB\")\n",
    "print(f\"\\n\" + \"=\n",
    ",\n",
    "All analyses complete! Generated files:\")\n",
    "print(\"=\n",
    ",\n",
    "  1. kg_visualization.html - Interactive network visualization\")\n",
    "print(f\"  2. top_25_connected_nodes.png - Top 25 nodes graph (PNG)\")\n",
    "print(f\"  3. top_25_connected_nodes.pdf - Top 25 nodes graph (PDF)\")\n",
    "print(f\"  4. graph_metrics_research_analysis.png - Metrics dashboard (PNG)\")\n",
    "print(f\"  5. graph_metrics_research_analysis.pdf - Metrics dashboard (PDF)\")\n",
    "print(f\"  6. centrality_comparison_degree_vs_betweenness.png - Centrality analysis\")\n",
    "print(f\"  7. {pdf_filename} - COMPREHENSIVE REPORT (Main deliverable)\")\n",
    "print(\"=\n",
    "\n",
    ": \n",
    ",\n",
    ": { \n",
    ": \n",
    " },\n",
    ": [\n",
    "════════════════════════════════════════════════════════════════════════════════\n",
    "ENVIRONMENT SETUP: Configure API Keys and Create Virtual Environment\n",
    "════════════════════════════════════════════════════════════════════════════════\n",
    "This cell sets up and creates the environment for LLM integrations.\n",
    "Configure your API keys below before running the multi-LLM extraction steps.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\n",
    ",\n",
    "ENVIRONMENT SETUP: LLM API Configuration\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    ",\n",
    "1\n",
    ",\n",
    ",\n",
    ",\n",
    "\\n1. SETTING ENVIRONMENT VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# OpenAI API Key (ChatGPT)\n",
    "OPENAI_API_KEY = \"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\"  # TODO: Replace with your OpenAI API key\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "    print(\"✓ OPENAI_API_KEY configured\")\n",
    "else:\n",
    "    print(\"○ OPENAI_API_KEY not configured (set above to enable ChatGPT)\")\n",
    "\n",
    "# Google API Key (Gemini)\n",
    "GOOGLE_API_KEY = \"AIzaSyACzafPCmSxWKxhksP4aErRQfT7nCymBxM\"  # TODO: Replace with your Google API key\n",
    "if GOOGLE_API_KEY:\n",
    "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "    print(\"✓ GOOGLE_API_KEY configured\")\n",
    "else:\n",
    "    print(\"○ GOOGLE_API_KEY not configured (set above to enable Gemini)\")\n",
    "\n",
    "# Anthropic API Key (Claude)\n",
    "ANTHROPIC_API_KEY = \"\"  # TODO: Replace with your Anthropic API key\n",
    "if ANTHROPIC_API_KEY:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "    print(\"✓ ANTHROPIC_API_KEY configured\")\n",
    "else:\n",
    "    print(\"○ ANTHROPIC_API_KEY not configured (set above to enable Claude)\")\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 2. CHECK CURRENT PYTHON ENVIRONMENT\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n2. PYTHON ENVIRONMENT INFORMATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"Current Directory: {os.getcwd()}\")\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 3. CHECK AND INSTALL REQUIRED PACKAGES\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n3. VERIFYING REQUIRED PACKAGES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "required_packages = {\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'networkx': 'networkx',\n",
    "    'pyvis': 'pyvis',\n",
    "    'reportlab': 'reportlab',\n",
    "    'openai': 'openai (for ChatGPT)',\n",
    "    'google.generativeai': 'google-generativeai (for Gemini)',\n",
    "    'anthropic': 'anthropic (for Claude)'\n",
    "}\n",
    "\n",
    "installed_packages = []\n",
    "missing_packages = []\n",
    "\n",
    "for import_name, package_name in required_packages.items():\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        installed_packages.append(package_name)\n",
    "        print(f\"✓ {package_name}\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(import_name)\n",
    "        print(f\"✗ {package_name} (not installed)\")\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 4. INSTALL MISSING PACKAGES\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n4. INSTALLING {len(missing_packages)} MISSING PACKAGES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Map import names to pip package names\n",
    "    pip_packages = {\n",
    "        'openai': 'openai',\n",
    "        'google.generativeai': 'google-generativeai',\n",
    "        'anthropic': 'anthropic'\n",
    "    }\n",
    "    \n",
    "    for import_name in missing_packages:\n",
    "        pip_name = pip_packages.get(import_name, import_name)\n",
    "        print(f\"Installing: {pip_name}...\", end=' ')\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pip_name])\n",
    "            print(\"✓\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "else:\n",
    "    print(f\"\\n4. ALL PACKAGES ALREADY INSTALLED\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"✓ No missing packages\")\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 5. CREATE ENVIRONMENT CONFIGURATION FILE\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n5. CREATING ENVIRONMENT CONFIGURATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "env_config = {\n",
    "    \"api_keys\": {\n",
    "        \"openai\": bool(OPENAI_API_KEY),\n",
    "        \"google\": bool(GOOGLE_API_KEY),\n",
    "        \"anthropic\": bool(ANTHROPIC_API_KEY)\n",
    "    },\n",
    "    \"python_version\": sys.version,\n",
    "    \"python_executable\": sys.executable,\n",
    "    \"installed_packages\": installed_packages,\n",
    "    \"missing_packages\": missing_packages,\n",
    "    \"environment_status\": \"Ready\" if not missing_packages else \"Partial\"\n",
    "}\n",
    "\n",
    "env_file = '.env_config.json'\n",
    "import json\n",
    "with open(env_file, 'w') as f:\n",
    "    json.dump(env_config, f, indent=2)\n",
    "\n",
    "print(f\"✓ Configuration saved to: {env_file}\")\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 6. ENVIRONMENT STATUS SUMMARY\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n6. ENVIRONMENT STATUS SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "api_available = sum([bool(OPENAI_API_KEY), bool(GOOGLE_API_KEY), bool(ANTHROPIC_API_KEY)])\n",
    "print(f\"LLM APIs Configured: {api_available}/3\")\n",
    "print(f\"Core Packages: {len(installed_packages)}/{len(required_packages)}\")\n",
    "\n",
    "if not missing_packages:\n",
    "    print(\"\\n✓ ENVIRONMENT READY - All packages installed\")\n",
    "    print(\"\\nTo use LLM features:\")\n",
    "    print(\"  1. Edit this cell and add your API keys (lines 25, 30, 35)\")\n",
    "    print(\"  2. Re-run this cell to activate the API keys\")\n",
    "    print(\"  3. Run the Multi-LLM Hidden Relationship Extraction step\")\n",
    "else:\n",
    "    print(f\"\\n⚠ ENVIRONMENT PARTIAL - {len(missing_packages)} packages missing\")\n",
    "\n",
    "# Store configuration in notebook kernel\n",
    "notebook_env = {\n",
    "    'api_keys_configured': api_available > 0,\n",
    "    'all_packages_installed': len(missing_packages) == 0,\n",
    "    'active_apis': []\n",
    "}\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    notebook_env['active_apis'].append('ChatGPT')\n",
    "if GOOGLE_API_KEY:\n",
    "    notebook_env['active_apis'].append('Gemini')\n",
    "if ANTHROPIC_API_KEY:\n",
    "    notebook_env['active_apis'].append('Claude')\n",
    "\n",
    "print(f\"\\nActive LLM Providers: {', '.join(notebook_env['active_apis']) if notebook_env['active_apis'] else 'None'}\")\n",
    "print(\"\\n\" + \"=\n",
    ",\n",
    "ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\n",
    "\n",
    ": \n",
    ",\n",
    ": { \n",
    ": \n",
    " },\n",
    ": [\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    ",\n",
    "=\n",
    ",\n",
    "STEP 9: MULTI-LLM HIDDEN RELATIONSHIP EXTRACTION\")\n",
    "print(\"Dual Type Analysis: Infrastructure Relationships + Operational Patterns\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    ",\n",
    "\\n1. INITIALIZING LLM PROVIDERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "llm_providers = {}\n",
    "active_provider = None\n",
    "extraction_method = \"pattern\"  # Default fallback\n",
    "\n",
    "# Try ChatGPT (OpenAI)\n",
    "try:\n",
    "    openai_key = os.environ.get('OPENAI_API_KEY')\n",
    "    if openai_key:\n",
    "        from openai import OpenAI\n",
    "        llm_providers['chatgpt'] = OpenAI(api_key=openai_key)\n",
    "        active_provider = 'chatgpt'\n",
    "        extraction_method = \"chatgpt\"\n",
    "        print(\"✓ ChatGPT (OpenAI) initialized successfully\")\n",
    "    else:\n",
    "        print(\"○ ChatGPT: OPENAI_API_KEY not set\")\n",
    "except Exception as e:\n",
    "    print(f\"○ ChatGPT: {str(e)}\")\n",
    "\n",
    "# Try Google Gemini\n",
    "try:\n",
    "    gemini_key = os.environ.get('GOOGLE_API_KEY')\n",
    "    if gemini_key:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=gemini_key)\n",
    "        llm_providers['gemini'] = genai\n",
    "        if not active_provider:\n",
    "            active_provider = 'gemini'\n",
    "            extraction_method = \"gemini\"\n",
    "        print(\"✓ Google Gemini initialized successfully\")\n",
    "    else:\n",
    "        print(\"○ Gemini: GOOGLE_API_KEY not set\")\n",
    "except Exception as e:\n",
    "    print(f\"○ Gemini: {str(e)}\")\n",
    "\n",
    "# Try Claude (Anthropic)\n",
    "try:\n",
    "    claude_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "    if claude_key:\n",
    "        from anthropic import Anthropic\n",
    "        llm_providers['claude'] = Anthropic(api_key=claude_key)\n",
    "        if not active_provider:\n",
    "            active_provider = 'claude'\n",
    "            extraction_method = \"claude\"\n",
    "        print(\"✓ Claude (Anthropic) initialized successfully\")\n",
    "    else:\n",
    "        print(\"○ Claude: ANTHROPIC_API_KEY not set\")\n",
    "except Exception as e:\n",
    "    print(f\"○ Claude: {str(e)}\")\n",
    "\n",
    "if not active_provider:\n",
    "    print(\"\\n⚠ No LLM API keys configured - Using PATTERN-BASED extraction\")\n",
    "else:\n",
    "    print(f\"\\n✓ Active LLM Provider: {active_provider.upper()}\")\n",
    "\n",
    "# Define TWO types of hidden relationships to extract\n",
    "RELATIONSHIP_TYPE_1 = \"Infrastructure Relationships\"  # C2 servers, IPs, infrastructure\n",
    "RELATIONSHIP_TYPE_2 = \"Operational Patterns\"           # Attack methodologies, workflows\n",
    "\n",
    "print(f\"\\nEXTRACTION FOCUS:\")\n",
    "print(f\"  Type 1: {RELATIONSHIP_TYPE_1}\")\n",
    "print(f\"  Type 2: {RELATIONSHIP_TYPE_2}\")\n",
    "\n",
    "# Find markdown annotation files\n",
    "print(\"\\n2. DISCOVERING CTI REPORTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "base_cti_path = os.path.dirname(data_path)\n",
    "annotation_path = os.path.join(base_cti_path, 'annotations')\n",
    "\n",
    "if not os.path.exists(annotation_path):\n",
    "    annotation_path = r'e:\\Knowledge Graph\\CTI-HAL\\annotations'\n",
    "\n",
    "markdown_files = []\n",
    "if os.path.exists(annotation_path):\n",
    "    for root, dirs, filenames in os.walk(annotation_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.md'):\n",
    "                markdown_files.append(os.path.join(root, filename))\n",
    "\n",
    "# Previously we limited to 15 for sampling; now process all files by default\n",
    "sample_files = markdown_files\n",
    "print(f\"Found {len(markdown_files)} markdown annotation files\")\n",
    "print(f\"Processing {len(sample_files)} CTI reports...\")\n",
    "\n",
    "# Store results by relationship type\n",
    "type1_relationships = []  # Infrastructure relationships\n",
    "type2_relationships = []  # Operational patterns\n",
    "\n",
    "for file_idx, file_path in enumerate(sample_files, 1):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            report_text = f.read()[:3000]\n",
    "        \n",
    "        if not report_text or len(report_text) < 100:\n",
    "            print(f\"\\n[{file_idx}/{len(sample_files)}] SKIP - {os.path.basename(file_path)} (too short)\")\n",
    "            continue\n",
    "        \n",
    "        file_parts = file_path.lower().split(os.sep)\n",
    "        actor = None\n",
    "        for part in file_parts:\n",
    "            if part.upper() in ['APT29', 'CARBANAK', 'FIN6', 'FIN7', 'OILRIG', 'SANDWORM', 'WIZARDSPIDER']:\n",
    "                actor = part.upper()\n",
    "                break\n",
    "        \n",
    "        if not actor:\n",
    "            print(f\"\\n[{file_idx}/{len(sample_files)}] SKIP - {os.path.basename(file_path)} (actor unknown)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{file_idx}/{len(sample_files)}] {actor} - {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # ═══ EXTRACTION METHOD A: LLM-Based ═══\n",
    "        if active_provider:\n",
    "            extracted = extract_with_llm(report_text, actor, active_provider, llm_providers)\n",
    "\n",
    "            # extracted may be None on error; or may contain empty lists\n",
    "            if extracted:\n",
    "                t1 = extracted.get('type1', [])\n",
    "                t2 = extracted.get('type2', [])\n",
    "                if (t1 or t2):\n",
    "                    type1_relationships.extend(t1)\n",
    "                    type2_relationships.extend(t2)\n",
    "                    print(f\"  ✓ LLM Extraction: {len(t1)} infrastructure + {len(t2)} patterns\")\n",
    "                    continue\n",
    "                else:\n",
    "                    # LLM returned empty results — log short raw preview and fallback to patterns\n",
    "                    raw_preview = extracted.get('raw', None)\n",
    "                    print(f\"  ○ LLM returned 0 relationships — falling back to pattern extraction\")\n",
    "                    if raw_preview:\n",
    "                        print(\"    LLM raw output preview:\")\n",
    "                        print(raw_preview[:400])\n",
    "                    # continue to pattern fallback below\n",
    "\n",
    "        # ═══ EXTRACTION METHOD B: Pattern-Based (Fallback) ═══\n",
    "        extracted = extract_with_patterns(report_text, actor)\n",
    "        type1_relationships.extend(extracted['type1'])\n",
    "        type2_relationships.extend(extracted['type2'])\n",
    "        if extracted['type1'] or extracted['type2']:\n",
    "            print(f\"  ✓ Pattern Extraction: {len(extracted['type1'])} infrastructure + {len(extracted['type2'])} patterns\")\n",
    "        else:\n",
    "            print(f\"  ○ No relationships detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "def extract_with_llm(text, actor, provider_name, providers):\n",
    "    \"\"\"Extract relationships using LLM and normalize returned schema.\n",
    "\n",
    "    The function accepts multiple LLM schemas and always returns a dict with\n",
    "    keys 'type1', 'type2' (lists) and 'raw' (original model text) when available.\n",
    "    If parsing or mapping fails, returns None so the pattern-based fallback can run.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extraction_prompt = f\"\"\"Analyze this CTI report and extract TWO types of hidden relationships:\n",
    "\n",
    "TYPE 1 - {RELATIONSHIP_TYPE_1}:\n",
    "- Specific IPs, domains, C2 servers mentioned\n",
    "- Infrastructure hosting patterns\n",
    "- Network infrastructure indicators\n",
    "\n",
    "TYPE 2 - {RELATIONSHIP_TYPE_2}:\n",
    "- Attack methodologies and workflows\n",
    "- Common attack patterns\n",
    "- Operational tactics and procedures\n",
    "\n",
    "REPORT TEXT:\n",
    "{text}\n",
    "\n",
    "Return JSON with this structure:\n",
    "{{\n",
    "  \"infrastructure\": [\n",
    "    {{\"value\": \"IP or domain\", \"purpose\": \"C2/hosting/etc\", \"actor\": \"{actor}\"}}\n",
    "  ],\n",
    "  \"patterns\": [\n",
    "    {{\"pattern_name\": \"attack type\", \"techniques\": [\"TECH1\", \"TECH2\"], \"actor\": \"{actor}\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON. Both arrays can be empty.\"\"\"\n",
    "\n",
    "        result_text = \"\"\n",
    "        if provider_name == 'chatgpt':\n",
    "            try:\n",
    "                response = providers['chatgpt'].chat.completions.create(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": extraction_prompt}],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=1500\n",
    "                )\n",
    "                result_text = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"    ChatGPT call error: {e}\")\n",
    "                return None\n",
    "            \n",
    "        elif provider_name == 'gemini':\n",
    "            try:\n",
    "                model = providers['gemini'].GenerativeModel('gemini-pro')\n",
    "                response = model.generate_content(extraction_prompt)\n",
    "                result_text = getattr(response, 'text', str(response))\n",
    "            except Exception as e:\n",
    "                print(f\"    Gemini call error: {e}\")\n",
    "                return None\n",
    "            \n",
    "        elif provider_name == 'claude':\n",
    "            try:\n",
    "                response = providers['claude'].messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20241022\",\n",
    "                    max_tokens=1500,\n",
    "                    messages=[{\"role\": \"user\", \"content\": extraction_prompt}]\n",
    "                )\n",
    "                # handle possible response shapes\n",
    "                if hasattr(response, 'content') and isinstance(response.content, list) and response.content:\n",
    "                    result_text = response.content[0].text\n",
    "                else:\n",
    "                    result_text = str(response)\n",
    "            except Exception as e:\n",
    "                print(f\"    Claude call error: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Try to extract JSON object from model output\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', result_text)\n",
    "        if not json_match:\n",
    "            print(\"    LLM output did not contain JSON; showing snippet:\")\n",
    "            print(result_text[:500])\n",
    "            return None\n",
    "\n",
    "        parsed = json.loads(json_match.group(0))\n",
    "\n",
    "        # Normalize possible key names to 'type1' and 'type2'\n",
    "        type1 = []\n",
    "        type2 = []\n",
    "\n",
    "        # Type 1 candidates\n",
    "        for k in ('type1', 'infrastructure', 'infrastructures', 'infra'):\n",
    "            if k in parsed and isinstance(parsed[k], list):\n",
    "                type1 = parsed[k]\n",
    "                break\n",
    "\n",
    "        # Type 2 candidates\n",
    "        for k in ('type2', 'patterns', 'pattern', 'operations'):\n",
    "            if k in parsed and isinstance(parsed[k], list):\n",
    "                type2 = parsed[k]\n",
    "                break\n",
    "\n",
    "        # Ensure entries are well-formed and include actor\n",
    "        def ensure_actor(entries):\n",
    "            normalized = []\n",
    "            for e in entries:\n",
    "                if not isinstance(e, dict):\n",
    "                    continue\n",
    "                if 'actor' not in e or not e['actor']:\n",
    "                    e['actor'] = actor\n",
    "                normalized.append(e)\n",
    "            return normalized\n",
    "\n",
    "        type1 = ensure_actor(type1)\n",
    "        type2 = ensure_actor(type2)\n",
    "\n",
    "        return {'type1': type1, 'type2': type2, 'raw': result_text}\n",
    "    except Exception as e:\n",
    "        print(f\"    LLM parsing error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_with_patterns(text, actor):\n",
    "    \"\"\"Extract relationships using pattern matching\"\"\"\n",
    "    results = {'type1': [], 'type2': []}\n",
    "    \n",
    "    # ═══ TYPE 1: Infrastructure Relationships ═══\n",
    "    \n",
    "    # IP addresses\n",
    "    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'\n",
    "    ips = list(set(re.findall(ip_pattern, text)))[:3]\n",
    "    for ip in ips:\n",
    "        results['type1'].append({\n",
    "            \"value\": ip,\n",
    "            \"purpose\": \"C2/hosting\",\n",
    "            \"actor\": actor\n",
    "        })\n",
    "    \n",
    "    # Domains (simple pattern)\n",
    "    domain_pattern = r'\\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\\.)+[a-z0-9]{2,6}\\b'\n",
    "    domains = list(set(re.findall(domain_pattern, text.lower())))[:2]\n",
    "    for domain in domains:\n",
    "        if not domain.endswith(('.md', '.pdf', '.txt', '.html')):\n",
    "            results['type1'].append({\n",
    "                \"value\": domain,\n",
    "                \"purpose\": \"domain/hosting\",\n",
    "                \"actor\": actor\n",
    "            })\n",
    "    \n",
    "    # ═══ TYPE 2: Operational Patterns ═══\n",
    "    \n",
    "    patterns_keywords = {\n",
    "        'Spear Phishing': ['spear phishing', 'targeted phishing', 'phishing email'],\n",
    "        'Credential Theft': ['credential', 'steal', 'password', 'auth'],\n",
    "        'Lateral Movement': ['lateral movement', 'move laterally', 'propagate'],\n",
    "        'Data Exfiltration': ['exfiltrate', 'data theft', 'steal data', 'download'],\n",
    "        'Persistence': ['persistence', 'maintain access', 'backdoor'],\n",
    "        'Privilege Escalation': ['privilege escalation', 'escalate', 'admin access'],\n",
    "        'Command & Control': ['command and control', 'c2', 'c&c', 'remote access'],\n",
    "        'Defense Evasion': ['evade', 'bypass', 'obfuscate', 'hiding']\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for pattern_name, keywords in patterns_keywords.items():\n",
    "        if any(kw in text_lower for kw in keywords):\n",
    "            results['type2'].append({\n",
    "                \"pattern_name\": pattern_name,\n",
    "                \"techniques\": [\"ATTACK_TECHNIQUE\"],\n",
    "                \"actor\": actor\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n3. AGGREGATING RESULTS BY RELATIONSHIP TYPE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nTYPE 1 - {RELATIONSHIP_TYPE_1}:\")\n",
    "print(f\"  Total discovered: {len(type1_relationships)}\")\n",
    "if type1_relationships:\n",
    "    unique_infra = {}\n",
    "    for rel in type1_relationships:\n",
    "        key = rel['value']\n",
    "        if key not in unique_infra:\n",
    "            unique_infra[key] = []\n",
    "        unique_infra[key].append(rel['actor'])\n",
    "    \n",
    "    for infra, actors in list(unique_infra.items())[:10]:\n",
    "        print(f\"    • {infra}\")\n",
    "        print(f\"      Used by: {', '.join(set(actors))}\")\n",
    "\n",
    "print(f\"\\nTYPE 2 - {RELATIONSHIP_TYPE_2}:\")\n",
    "print(f\"  Total discovered: {len(type2_relationships)}\")\n",
    "if type2_relationships:\n",
    "    unique_patterns = {}\n",
    "    for rel in type2_relationships:\n",
    "        pattern = rel['pattern_name']\n",
    "        if pattern not in unique_patterns:\n",
    "            unique_patterns[pattern] = []\n",
    "        unique_patterns[pattern].append(rel['actor'])\n",
    "    \n",
    "    for pattern, actors in sorted(unique_patterns.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "        print(f\"    • {pattern}\")\n",
    "        print(f\"      Used by: {', '.join(set(actors))}\")\n",
    "\n",
    "# 4. SAVE BOTH TYPES\n",
    "print(\"\\n4. SAVING EXTRACTED RELATIONSHIPS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "combined_relationships = {\n",
    "    \"extraction_method\": extraction_method,\n",
    "    \"type1_name\": RELATIONSHIP_TYPE_1,\n",
    "    \"type1_relationships\": type1_relationships,\n",
    "    \"type2_name\": RELATIONSHIP_TYPE_2,\n",
    "    \"type2_relationships\": type2_relationships,\n",
    "    \"total_type1\": len(type1_relationships),\n",
    "    \"total_type2\": len(type2_relationships)\n",
    "}\n",
    "hidden_rel_file = 'hidden_relationships_dual_type.json'\n",
    "with open(hidden_rel_file, 'w') as f:\n",
    "    json.dump(combined_relationships, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Saved to: {hidden_rel_file}\")\n",
    "\n",
    "# 5. VISUALIZE BOTH TYPES\n",
    "print(\"\\n5. CREATING DUAL-TYPE VISUALIZATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Panel 1: Type 1 by Actor\n",
    "type1_by_actor = {}\n",
    "for rel in type1_relationships:\n",
    "    actor = rel['actor']\n",
    "    type1_by_actor[actor] = type1_by_actor.get(actor, 0) + 1\n",
    "\n",
    "if type1_by_actor:\n",
    "    actors1 = list(type1_by_actor.keys())\n",
    "    counts1 = list(type1_by_actor.values())\n",
    "    ax1.bar(actors1, counts1, color='#FF6B6B', edgecolor='black', linewidth=1.2)\n",
    "    ax1.set_ylabel('Count', fontweight='bold')\n",
    "    ax1.set_title(f'Type 1: {RELATIONSHIP_TYPE_1}\\n(by APT Group)', fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No Type 1 relationships found', ha='center', va='center')\n",
    "    ax1.set_title(f'Type 1: {RELATIONSHIP_TYPE_1}', fontweight='bold')\n",
    "\n",
    "# Panel 2: Type 2 by Actor\n",
    "type2_by_actor = {}\n",
    "for rel in type2_relationships:\n",
    "    actor = rel['actor']\n",
    "    type2_by_actor[actor] = type2_by_actor.get(actor, 0) + 1\n",
    "\n",
    "if type2_by_actor:\n",
    "    actors2 = list(type2_by_actor.keys())\n",
    "    counts2 = list(type2_by_actor.values())\n",
    "    ax2.bar(actors2, counts2, color='#4ECDC4', edgecolor='black', linewidth=1.2)\n",
    "    ax2.set_ylabel('Count', fontweight='bold')\n",
    "    ax2.set_title(f'Type 2: {RELATIONSHIP_TYPE_2}\\n(by APT Group)', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No Type 2 relationships found', ha='center', va='center')\n",
    "    ax2.set_title(f'Type 2: {RELATIONSHIP_TYPE_2}', fontweight='bold')\n",
    "\n",
    "# Panel 3: Type 1 Distribution (Infrastructure purposes)\n",
    "type1_purposes = {}\n",
    "for rel in type1_relationships:\n",
    "    purpose = rel.get('purpose', 'unknown')\n",
    "    type1_purposes[purpose] = type1_purposes.get(purpose, 0) + 1\n",
    "\n",
    "if type1_purposes:\n",
    "    purposes = list(type1_purposes.keys())\n",
    "    counts = list(type1_purposes.values())\n",
    "    colors_type1 = ['#FF6B6B', '#FFA07A', '#FFB6C1'][:len(purposes)]\n",
    "    ax3.pie(counts, labels=purposes, autopct='%1.1f%%', colors=colors_type1, startangle=90)\n",
    "    ax3.set_title(f'{RELATIONSHIP_TYPE_1} Breakdown', fontweight='bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No Type 1 data', ha='center', va='center')\n",
    "\n",
    "# Panel 4: Type 2 Distribution (Pattern types)\n",
    "type2_patterns = {}\n",
    "for rel in type2_relationships:\n",
    "    pattern = rel.get('pattern_name', 'unknown')\n",
    "    type2_patterns[pattern] = type2_patterns.get(pattern, 0) + 1\n",
    "\n",
    "if type2_patterns:\n",
    "    patterns = list(type2_patterns.keys())[:6]  # Top 6\n",
    "    counts = [type2_patterns[p] for p in patterns]\n",
    "    colors_type2 = ['#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DFE6E9', '#A29BFE'][:len(patterns)]\n",
    "    ax4.barh(patterns, counts, color=colors_type2, edgecolor='black', linewidth=1.2)\n",
    "    ax4.set_xlabel('Count', fontweight='bold')\n",
    "    ax4.set_title(f'{RELATIONSHIP_TYPE_2} Types', fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No Type 2 data', ha='center', va='center')\n",
    "\n",
    "plt.suptitle('Dual-Type Hidden Relationship Extraction Analysis', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "hidden_rel_image = 'hidden_relationships_dual_type.png'\n",
    "plt.savefig(hidden_rel_image, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Visualization saved to: {hidden_rel_image}\")\n",
    "print(f\"✓ Extraction Method Used: {extraction_method.upper()}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\n",
    ",\n",
    "STEP 9 COMPLETE: Dual-Type Hidden Relationship Extraction\")\n",
    "print(\"=\n",
    "\n",
    ": \n",
    ",\n",
    ": { \n",
    ": \n",
    " },\n",
    ": [\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "print(\"=\n",
    ",\n",
    "STEP 10: RESEARCH KG VALIDATION — PATH ANALYSIS & LINK PREDICTION\")\n",
    "print(\"=\n",
    ",\n",
    ",\n",
    ",\n",
    "    raise\n",
    "\n",
    "undirected = G.to_undirected()\n",
    "\n",
    "# Heuristics to detect phishing technique nodes and infrastructure nodes\n",
    "ip_re = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "domain_re = re.compile(r\"\\b(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\\.)+[a-z]{2,6}\\b\", re.I)\n",
    "\n",
    "def is_phishing_node(n):\n",
    "    s = str(n).lower()\n",
    "    label = str(G.nodes[n].get('label','')).lower() if isinstance(G.nodes[n], dict) else ''\n",
    "    return ('phish' in s) or ('phish' in label) or ('spear' in s and 'phish' in label)\n",
    "\n",
    "def is_infra_node(n):\n",
    "    s = str(n)\n",
    "    label = str(G.nodes[n].get('label','')) if isinstance(G.nodes[n], dict) else ''\n",
    "    if ip_re.search(s) or ip_re.search(label):\n",
    "        return True\n",
    "    if domain_re.search(s) or domain_re.search(label):\n",
    "        return True\n",
    "    low = (s+label).lower()\n",
    "    return any(k in low for k in ['c2', 'command', 'control', 'infrastructure', 'host', 'server'])\n",
    "\n",
    "# Detect nodes\n",
    "phish_nodes = [n for n in G.nodes() if is_phishing_node(n)]\n",
    "infra_nodes = [n for n in G.nodes() if is_infra_node(n)]\n",
    "\n",
    "print(f\"Detected phishing nodes: {len(phish_nodes)}\")\n",
    "print(f\"Detected infrastructure-like nodes: {len(infra_nodes)}\")\n",
    "\n",
    "# PATH ANALYSIS: find shortest paths between phishing techniques -> infra\n",
    "path_counts = Counter()\n",
    "path_examples = defaultdict(list)\n",
    "max_path_len = 8\n",
    "\n",
    "for s in phish_nodes:\n",
    "    for t in infra_nodes:\n",
    "        if s == t:\n",
    "            continue\n",
    "        try:\n",
    "            path = nx.shortest_path(G, source=s, target=t)\n",
    "            if 1 < len(path) <= max_path_len:\n",
    "                key = ' -> '.join(map(str, path))\n",
    "                path_counts[key] += 1\n",
    "                if len(path_examples[key]) < 3:\n",
    "                    path_examples[key].append({'source': s, 'target': t, 'path': path})\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# Top recurring paths (by distinct pair count)\n",
    "top_paths = path_counts.most_common(10)\n",
    "\n",
    "print('\\nTop recurring Phishing -> Infra paths:')\n",
    "if not top_paths:\n",
    "    print('  No paths found from detected phishing nodes to infra nodes in the graph.')\n",
    "else:\n",
    "    for p, cnt in top_paths:\n",
    "        print(f'  • Count={cnt}: {p}')\n",
    "\n",
    "# Research question 1: Infrastructure reuse across APT groups\n",
    "# Heuristic: inspect node attributes for an 'actor' list or edge attributes; otherwise use neighborhood mapping\n",
    "infra_usage = defaultdict(set)\n",
    "for infra in infra_nodes:\n",
    "    # gather neighboring actor nodes if present\n",
    "    for nbr in G.neighbors(infra):\n",
    "        # heuristic: actor nodes often contain 'APT' or known actor names\n",
    "        name = str(nbr)\n",
    "        if any(a in name.upper() for a in ['APT', 'CARBANAK', 'FIN6', 'FIN7', 'OILRIG', 'SANDWORM', 'WIZARDSPIDER']):\n",
    "            infra_usage[infra].add(name.upper())\n",
    "    # also check node attribute 'actor' or 'actors'\n",
    "    attrs = G.nodes[infra],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3527d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a2d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cde80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc68b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71bcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aacb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551af1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3eb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd39797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d5ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abbebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86a572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e3350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377aad18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0d5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6741c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd3abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7feb54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e12b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700799f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0741e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f2dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad1dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd3f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7aa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce047ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c6fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0d5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1323c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3eb2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97328290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5b854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b0340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866924af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369a8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96b624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360409dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3571e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13b459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10bf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c051b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49d001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa308e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5094ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d8d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c34cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a05ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b828e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2debf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e09bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5b511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f029749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550107c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bbfbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ecd4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6cbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778161d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14863b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc7e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9da28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be507e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ae56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae620575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008d27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1e002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f253d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b68d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383dfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f81196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
